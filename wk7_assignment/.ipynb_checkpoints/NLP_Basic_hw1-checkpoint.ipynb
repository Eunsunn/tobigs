{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the woman is a wise queen',\n",
    "          'the man is a wise president',\n",
    "          'she is a pretty woman',\n",
    "          'he is a strong man',\n",
    "          'she is still young',\n",
    "          'he is very old',\n",
    "          'he is the current president of US',\n",
    "          'the prince is a son of the king',\n",
    "          'the princess is a daughter of the king',\n",
    "          'a prince is a young man',\n",
    "          'a princess is a young woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1) 주석 부분에 function1의 목적을 쓰고, 그 주된 목적에 맞게 function1의 이름 변경하기\n",
    "\n",
    "# 중심단어의 주변단어를 찾는 함수\n",
    "def neighbor_words(corpus):    \n",
    "    sentences = []\n",
    "    for sentence in corpus:\n",
    "        sentences.append(sentence.split()) #문장을 단어단위로 쪼개기\n",
    "        # [['the', 'woman', 'is', 'a', 'wise', 'queen'],\n",
    "        #  ['the', 'man', 'is', 'a', 'wise', 'president']] : sentences 리스트는 이렇게 생겼다!\n",
    "\n",
    "    window_size = 2 # q2) s의 의미 : window_size\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences: # 문장마다 for문\n",
    "        for idx, word in enumerate(sentence): # 한 문장의 단어와 단어의 인덱스\n",
    "            for neighbor in sentence[max(idx - window_size, 0) : min(idx + window_size, len(sentence)) + 1] :\n",
    "                # max(idx - s, 0):이웃하는 단어들의 처음 인덱스, min(idx + s, len(sentence)) + 1:이웃하는 단어들의 마지막 인덱스\n",
    "                if neighbor != word: #중심 단어의 이웃하는 단어들만 추가\n",
    "                    data.append([word, neighbor]) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = neighbor_words(corpus)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3) 주석 부분에 function2의 목적을 쓰고, 그 목적에 맞게 function2의 이름 변경하기\n",
    "\n",
    "#중복없이 단어 목록을 만드는 함수\n",
    "def word_set(corpus):\n",
    "    words = []\n",
    "    for text in corpus: # 문장마다 for문\n",
    "        for word in text.split(' '): # 한 문장의 단어마다 for문돌려서 word 리스트에 추가\n",
    "            words.append(word) \n",
    "    words = set(words) #중복을 제거한 단어 집합\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_set(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4) 주석 부분에 function3의 목적을 쓰고, 그 목적에 맞게 function3과 d의 이름을 변경하기\n",
    "\n",
    "# 단어에 번호를 붙여주는 함수({단어:인덱스} 딕셔너리)\n",
    "def word_numbering(words):\n",
    "    idx = {}\n",
    "    for i,word in enumerate(words):\n",
    "        idx[word] = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = word_numbering(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# q5) 주석 부분에 function4의 목적을 쓰고, 그 목적에 맞게 function4와 ohe의 이름 변경하기\n",
    "\n",
    "# tokenizing한 하나의 단어를 one-hot encoding하는 함수\n",
    "# word_idx에는 단어의 인덱스(word_numbering 함수의 values를 받음), dim(차원)은 단어개수(len(words))\n",
    "def one_hot_encoding(word_index, ONE_HOT_DIM):\n",
    "    encoding = np.zeros(ONE_HOT_DIM)\n",
    "    encoding[word_index] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q6) Word2Vec을 tensorflow로 구현한 코드에서 ? 부분을 올바르게 채워넣기\n",
    "# q7) 여기서 구현한 Word2Vec의 아키텍쳐는 CBOW or Skip Gram ?\n",
    "\n",
    "# Skip Gram이다!\n",
    "\n",
    "# cross entropy 참고\n",
    "# https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/\n",
    "# https://kevinthegrey.tistory.com/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ONE_HOT_DIM = len(words) # 인코딩 차원 = 단어개수(중복x)\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "#모든 중심딘어, 주변단어 쌍을 encoding\n",
    "for x, y in zip(df['input'], df['label']): #input = 중심단어, label = 주변단어\n",
    "    X.append(one_hot_encoding(d[x], ONE_HOT_DIM)) #d : 단어를 numbering한 딕셔너리({단어:인덱스}) #d[x] : x의 인덱스(value)\n",
    "    Y.append(one_hot_encoding(d[y], ONE_HOT_DIM))\n",
    "\n",
    "# convert X,Y to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "# placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "\n",
    "# embedding dimension\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# hidden layer : represent word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM])) \n",
    "b1 = tf.Variable(tf.random_normal([1]))\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM])) # 라벨 인코딩차원으로 나와야함!\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "output = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function : cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(output), axis=[1]))\n",
    "\n",
    "# training\n",
    "train = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 10000\n",
    "for i in range(iteration):\n",
    "    # input : X_train which is one hot encoded word\n",
    "    # label : Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 2000 == 0:\n",
    "        print('iteration '+ str(i) +' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hidden layer (W1 + b1) -> look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2 ))\n",
    "    \n",
    "PADDING = 1.0\n",
    "x1_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "x2_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x1_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "x2_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x1_axis_min,x1_axis_max)\n",
    "plt.ylim(x2_axis_min,x2_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
